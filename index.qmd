---
title: "Differential Abundance anaysis"
author: "Sanyukta Chapagain"
format: html
editor: visual
---

## Differential Abudance analysis on 16S RNA data from the Paper [(Nejman et al., 2020)](https://www.science.org/doi/10.1126/science.aay9189)

## 1. What is Differential Abundance (DA) Analysis?

Differential Abundance Analysis (DAA) is the use of various statistical methods to identify significant differences in the abundances of individual taxa between two or more comparison groups.

Microbiome data possesses unique characteristics that contribute to the complexity of its analysis.To address these inherent data challenges, different DAA tools and packages operate based on distinct statistical assumptions or computational frameworks. The choice of package is therefore critical, as each is specifically designed to handle certain data features more effectively than others. Some of the data characteristics of a microbiome data includes:

#### a) Compostionality:

Compositionality is the most important characteristic of microbiome data. The ASV/OTU count, which is the main input used in Differential Abundance (DA) analysis, represents **relative abundance**; that is, the count is proportional to the total number of sequencing reads (library size) per sample and not the true absolute count for that particular taxon. Consequently, an apparent increase in one taxon's relative count naturally results in a decrease in the relative abundance of another taxon's count. This is a significant problem because working solely with relative abundance often does not reflect the true absolute count of the species in our data and can therefore lead to a loss of biological significance..

#### b) Sparisty

Microbiome datasets feature a large number of taxa (OTUs/ASVs) compared to the smaller number of samples we work with, which impacts the statistical modeling as there are more features than samples. An important point to note about sparse data is that sometimes the taxon is genuinely **not present** (a structural zero), hence the count is zero, but sometimes the zero count could also be a technical error in the sequencing and the taxon was simply not detected because it had low abundance (a sampling zero).

#### c) Overdispersion

In the count data, the **variance of counts** for a given taxon is significantly larger than its mean. This characteristic is known as over-dispersion, which violates the assumptions of a simple distribution (like the Poisson distribution) often used in count data.

## 2. What is the analysis in Nejman's Paper ,

The paper characterized the tumor-associated microbiome across 7 cancer types using the 16s rRNA sequencing and statistical diversity analysis by looking into the community difference through alpha and beta diveristy. They found out that breast cancer had the most diverse microbiome profile

What I am doing in this analysis is re-analyzing the same dataset with four established DA methods - ALDEx2, ANCOM-II , DESeq2 and MaAsLin2. This will help identify taxa that differ significantly between tumor (T) vs Normal adjacent tumor (NAT). This will allow comparision of the DA tools and also study if these packages reproduce the study's key finding that the breast tumor had the richest and most diverse microbiome profile.

Our use of multiple differential abundance (DA) tools follows the **consensus approach** recommended by [(Nearing et al., 2022)](#0). In their study, the authors systematically evaluated 14 DA methods across 38 microbiome datasets and concluded that, because each method relies on different statistical assumptions, the results can vary substantially. Therefore, they recommended using a consensus strategy examining the overlapping significant features (ASVs) across multiple DA tools to obtain more reliable biological interpretations.n

Based on these findings, we selected **ALDEx2** and **ANCOM-II**, which were identified as the **most conservative and consistent** tools, as well as **MaAsLin2** and **DESeq2**. Although **DESeq2** was originally designed for RNA-seq data and was reported by Nearing et al. to produce relatively higher false discovery rates (FDR) in microbiome applications, we included it for comparison because [(Calgaro et al., 2020)](https://genomebiology.biomedcentral.com/articles/10.1186/s13059-020-02104-1) found DESeq2 to be among the top-performing methods overall. Given this contrast in reported performance, including DESeq2 in our analysis allows us to evaluate how it behaves on this dataset relative to the other DA frameworks.

# 3 Workflow

The main data that we will be working with in this study is the supplementary table titled **Table S2. Normalized reads of all samples in study with clinical data.***.*

![](images/clipboard-1108813451.png)

This table is taken from Nejman et al. (2020) and contains the normalized read counts for all tumor and control samples along with their associated clinical information. Nejman’s study applied a six-stage filtering process to remove potential contaminants and batch-related artifacts; however, this version of Table S2 represents the data before any filtering was applied

The table has two main sections. The upper part contains the clinical metadata for all samples in the study (such as sample ID, tissue type, patient information, sequencing batch, and clinical characteristics). The lower part contains the prevalence table for all taxa detected across samples, including their taxonomic classifications from domain down to species level.

The row labeled “Total \# normalized reads without general contaminants” represents the total normalized read count per sample *after* contamination filtering would be applied. However, in this table, the data are shown before filtering, meaning all reads are still included.

## 3.1 Data cleaning

### 3.1.01 Splitting into clincal and prevalence table

The first step in our analysis was splitting the table into a prevalence.csv and a clinical.csv file. In the clinical table, we added a new column called Barcodes, which represents the sequencing sample IDs. The Sample_ID (WIS) in the clinical data were numeric, while in the prevalence table, the sample IDs were in a format such as `2015-11-17 - Hiseq-4/BC11_GTATAACA`. We matched each barcode with its corresponding WIS Sample ID to ensure proper mapping and alignment between the two tables during the DA step.

### 3.1.02 Removing Bact ID \>= 30,000

In Table S2, at the bottom of the prevalence table, we had BactID values ≥ 30,000, which did not represent species-level data. The corresponding prevalence values for these rows represented higher taxonomic levels, such as genus or family. Therefore, all rows with BactID ≥ 30,000 were removed from the prevalence table, as they do not reflect species-level abundance and saving the file as prevalence_after_bactID_01.csv

### 3.1.03 Prevalence filtering

In this step, we removed species identified as likely contaminants from lab procedures. Low biomass tumor samples are prone to contamination from DNA extraction kits, PCR reagents, paraffin embedding, and other sources. Following the paper’s Filter 1 method, we removed any species that appeared in more than 7.5% of DNA extraction/NTC negative controls or more than 7.5% of paraffin embedding controls. These two values were found in the first two columns of the prevalence table, named “Prevalence in DNA extraction/NTC negative controls” and “Prevalence in Paraf. Controls.” The 7.5% threshold was chosen as the inflection point in the bimodal distribution, where species below this value were considered real detections and those above were likely contaminants. We converted the percentage strings to numeric values and applied an OR rule to remove species that exceeded 7.5% in either control type.

After filtering, we removed 168 contaminant species out of 11,211 (1.5%) and kept 11,043 clean species (98.5%). The resulting table was saved as prevalence_no_contaminants_02.csv, which matches the 168 species reported as removed in Filter 1 of the paper.

### 3.1.04 Format coversion - into relative and integer count

The original Table S2 is described as the normalized reads of all samples in the study with clinical data. It also includes two rows named total normalized reads and total normalized reads without general contaminants. These rows summarize the overall read counts per sample but still include taxa with BactID values greater than 30,000, which are higher-level taxa (not species).

Because of this, we did not use those total normalized read values directly. Instead, after filtering out all rows with BactID ≥ 30,000, we recalculated the total normalized reads for each sample using the filtered prevalence table. This allowed us to obtain accurate totals that represent only species-level data.

From this recalculated data, we created two separate versions for different DA tools.

For the **integer count format**, we rounded the normalized read values (floats) to the nearest whole number, since count-based tools like DESeq2 require integer inputs.

For the **relative abundance format**, we divided each sample’s read values by its corresponding total normalized reads, producing proportions that sum to approximately 1.0 per sample.

This conversion step was necessary because some DA tools operate on count data, while others are designed for compositional or relative abundance data. The files were saved as prevalence_relative_abundance_all_csv and prevalence_integer_counts_all.csv.

## 3.2 Cancer specific data extraction

### 3.2.01 Breast Cancer data

#### Clincal data:

In this step, we extracted cancer-specific data for tumor and normal tissue comparisons. We selected three cancer types for the DA analysis: breast, ovary, and lung. The other cancers (melanoma, pancreas, GBM, and bone) were excluded because they lacked normal or NAT samples.

For the breast dataset, we first extracted the breast-specific clinical data and saved it as **breast_clinical.csv**. We then created another file called **breast_clinical_aligned.csv**, which included the corresponding barcodes for each sample. The barcodes were extracted and matched with the Sample_ID (WIS) values to ensure that each sample in the clinical file correctly aligned with the sample IDs in the prevalence table. The alignment was performed using the Jupyter source file named **alignment_breast.ipynb**, from which the breast_clinical_aligned.csv file was generated. This files contains all breast cancer sample that is N , NAT and T. This alignment step ensured that the metadata and prevalence data were perfectly matched before performing any downstream analysis.

#### Prevalence data : Applying 3 % prevalence filtering

Nearing et al's paper also recommneds applying a prevalence filtering of the data before we proceed to do a DA analysis. They tried it with a 10 % filtering , but because our data is a low biomass data , we only proceeded to use a 3% filtering threshold

In this step, we performed cancer-specific prevalence filtering using the 3% threshold. From the prevalence table, we used the two files prevalence_relative_abundance_all.csv and prevalence_integer_counts_all.csv.

From each of these, we extracted the breast cancer samples. After extracting the breast data for both relative and integer formats, we applied the 3% prevalence filtering, where a species had to be present in at least 3% of the samples to be kept. Once the filtering was applied, the resulting files were saved as breast_relative_filtered.csv and breast_integers_filtered.csv. This process was done using the Jupyter source file named **breast_cancer_RandI.ipynb**.

### 3.2.01 Creating R file without error and DA ready

In this step, we prepared the breast cancer data for differential abundance analysis by creating standardized input files compatible with all DA tools. This was done using the Python script named prepare_DA_input.ipynb.

From the previously filtered breast data, we kept only the Tumor and NAT samples and removed all other tissue types such as Normal and Fibroadenoma. This filtering was necessary because the DA analysis focuses on comparing Tumor vs NAT. We also analyzed possible confounding factors such as DNA extraction batch, sequencing center, and patient pairing, which are summarized in the DA_design_info.txt file.

To make the dataset compatible with R, we created a new column called Sample_ID_clean. In the original dataset, sample names contained characters like spaces, dashes, and slashes (for example, 2015-11-17 - Hiseq-4/RDB102_TGCCTCAA), which caused issues when loading the data in R. These were replaced with underscores to create a consistent, R-friendly sample ID format. This cleaned sample ID was used as the main identifier to align the metadata and prevalence matrices.

The resulting files included DA_metadata.csv, DA_counts_integer.csv, and DA_counts_relative.csv, along with DA_design_info.txt, which documents the final design formula and any data limitations. These files are the DA-ready versions that were later used in R for differential abundance analysis with tools such as DESeq2, MaAsLin2, ALDEx2, and ANCOM-II.

**It is important to know that the filtered DA_metadata along with the DA_counts and DA integers contains only NAT vs T samples. Some of the NAT and T samples came from the same patients in the Nejman's study . We call them as Paired patient which can be added as design in some DA tools so there is no paired Patient bias in the statistical analysis So the DA_metadata has both paired and unpaired NAT and T samples.**

### 3.2.02 Confounding factor analysis

Confounding analysis was performed to identify variables that might bias the observed differences in bacterial abundance between tumor and NAT samples. This step is essential before running differential abundance analysis because certain technical or biological factors (like sequencing batch or center) can artificially influence results. After testing all available covariates using the script ,confounding_BC.ipynb, , we found that some were confounded and therefore excluded from the model. As a result, only Center and Group were used as fixed effects for downstream DA tools, while Patient_ID was included as a random effect in models that support random effects (such as MaAsLin2).

## 3.3 Differential Abundance Analysis

### 3.3.01 DESEq2

DESeq2 is a count-based tool originally developed for RNA-seq and is adapted for microbiome data. It handles overdispersion in counts, allows for covariate adjustment (such as Center), but cannot handle random effects (e.g., patient pairing).

DESeq2 uses a **negative binomial model** for count data:

$$
K_{ij} \sim NB(\mu_{ij}, \alpha_i)
$$

where:

-   $K_{ij}$ is the observed count for feature $i$ in sample $j$
-   $\mu_{ij}$ is the expected mean
-   $\alpha_{i}$ is the dispersion parameter

The **design formula** used was:

$$
\log(\mu_{ij}) = \beta_0 + \beta_{\text{Center}} + \beta_{\text{Group}}
$$

where:

-   $\beta_0$ is the intercept\
-   $\beta_{\text{Center}}$ accounts for center-specific effects\
-   $\beta_{\text{Group}}$ tests the effect of **Tumor vs NAT**, the main variable of interest

**Fixed Effects:**\
All variables in the formula (\~ Center + Group) are treated as fixed effects. DESeq2 estimates the effect of each variable, for example:

-   How does being in NAT vs Tumor affect bacterial abundance ?

-   How does being at Sheba vs Maccabi (center names ) affect abundance?

**Why Patient_ID Was Not Included:**\
There are too many Patient_ID levels (154 paired patients), leading to a “not full rank” error. DESeq2 cannot separate patient and group effects in a perfectly paired or mixed design.

#### Load the Data

```{r}
#| eval: false

library(DESeq2)

# Load with check.names=FALSE to preserve sample IDs
metadata <- read.csv("DA_metadata.csv", row.names=1, check.names=FALSE)
counts_full <- read.csv("DA_counts_integer.csv", check.names=FALSE)

# Separate taxonomy from counts
taxonomy_cols <- c("Prevalence in DNA extraction/NTC negative controls",
                   "Prevalence in Paraf. Controls",
                   "BactID", "domain", "phylum", "class", "order", 
                   "family", "genus", "species")

taxonomy <- counts_full[, taxonomy_cols]
counts <- counts_full[, !names(counts_full) %in% taxonomy_cols]
rownames(counts) <- taxonomy$BactID

```

#### Prepare Metadata

```{r}
#| eval: false

# Set Group factor with Tumor as reference
metadata$Group <- factor(metadata$Group, levels=c("Tumor", "NAT"))
metadata$Center <- factor(metadata$Center)

# Verify alignment
if (!identical(colnames(counts), rownames(metadata))) {
  stop("ERROR: Sample order doesn't match!")
}

```

We use tumor as reference when comparing NAT vs T

#### Creating DESeq2 object

```{r}
#| eval: false

dds <- DESeqDataSetFromMatrix(
  countData = counts,
  colData = metadata,
  design = ~ Center + Group
)

```

This design adjusts for Center effect before testing Group effect

#### Estimate Size factors

```{r}
#| eval: false

dds <- estimateSizeFactors(dds, type="poscounts")

```

Poscounts works with sparse data and the deseq's default method - geometric mean fails with it. poscounts uses modified geometric mean that handles zeros in microbome data.

#### Run DESEQ

```{r}
#| eval: false
dds <- DESeq(dds)


```

This performs

1.  Size factor normalization

2.  Dispersion estimation (gene-wise, then shrinkage)

3.  Negative binomial GLM fitting

4.  Wald test for significance

#### Extract Results

```{r}
#| eval: false

res <- results(dds, contrast=c("Group", "NAT", "Tumor"))
```

-   Format: `c("variable", "numerator", "denominator")`

<!-- -->

-   `c("Group", "NAT", "Tumor")` = NAT vs Tumor

<!-- -->

-   log2FC \> 0 = higher in NAT

<!-- -->

-   log2FC \< 0 = higher in Tumor

#### Process results

```{r}
#| eval: false

results_df <- as.data.frame(res)
results_df$BactID <- rownames(results_df)
results_final <- merge(results_df, taxonomy, by="BactID")
results_final <- results_final[order(results_final$pvalue), ]


# All results
write.csv(results_final, "deseq2_results.csv", row.names=FALSE)

# Significant only
sig_005 <- results_final[results_final$padj < 0.05 & !is.na(results_final$padj), ]
sig_010 <- results_final[results_final$padj < 0.1 & !is.na(results_final$padj), ]

if (nrow(sig_005) > 0) {
  write.csv(sig_005, "deseq2_significant_padj005.csv", row.names=FALSE)
}
if (nrow(sig_010) > 0) {
  write.csv(sig_010, "deseq2_significant_padj010.csv", row.names=FALSE)
}



```

### Understanding output

![](images/clipboard-1244530020.png)

**`baseMean`**\
Mean normalized count across all samples.\
→ Higher values indicate more abundant bacteria.

**`log2FoldChange`**\
Log₂ fold change (NAT vs Tumor).\
→ Positive = enriched in NAT\
→ Negative = enriched in Tumor\
→ \|2\| = doubling/halving

**`lfcSE`**\
Standard error of the log₂ fold change.\
→ Smaller values mean a more precise estimate.

**`stat`**\
Wald test statistic.\
→ Calculated as `log2FoldChange / lfcSE`\
→ Larger magnitude = stronger evidence for differential abundance.

**`pvalue`**\
Raw p-value.\
→ Values \< 0.05 are typically considered significant.

**`padj`**\
Benjamini-Hochberg (BH) adjusted p-value.\
→ \< 0.05 = significant after correcting for multiple testing

### 3.3.03 MaAsLin2

In MaAsLin , we can handle the patient pairing using random effect. MaAsLin uses a linear mixed model framework which can adjust for both the covariates and paired patient issue. This package used relative abudnance count.

## Statistical Framework

MaAsLin2 fits a **linear mixed model** for each species:

$$
\log(\text{abundance}_{ij}) = \beta_0 + \beta_{\text{Center}} + \beta_{\text{Group}} + u_{\text{patient}} + \varepsilon
$$

where:

-   $\beta_0$, $\beta_{\text{Center}}$, $\beta_{\text{Group}}$: fixed effects (including Center and Group/Tumor status)
-   $u_{\text{patient}}$: random effect for patient-specific variation (accounts for pairing)
-   $\varepsilon$: residual error

### Misc: Fixed effect and Random effect

**Fixed effects** are These are the **systematic variables** the ones whose specific effects you *want to estimate or test*.

In the above formula, **Center** controls for *batch effects or site differences*.

-   Example question: *Do samples from different hospitals show systematic differences in abundance due to lab or sequencing variation?*

    **Group** Tests the biological contrast of interest (Tumor vs NAT).

    Example question: *Is this bacterium more abundant in tumor tissue than in NAT tissue?*

So, fixed effects represent **predictors with consistent, reproducible influence** across your dataset — they’re not random noise; they’re what you actually *want to measure*

**Random effects** accounts for **random variability** that you *don’t want to test directly* but *must control for*, especially in paired or repeated-measures designs

This is because conceptually , each patient contribute two samples as they are paired designs (NAT + T). Now obviously, these two tissue samples from the same patient would be more similar to each other than to other patients samples. This similarity violtaes the assumption of independent samples.

So, models like MaAsLin adds a random intercept per patient which means that every patient now has their own baseline abundance level. The model doesn’t care *which* patient has which intercept ,it only cares about the *variance* among patients (σ²_patient). This "absorbs" the within-patient correlation, preventing it from inflating your fixed-effect estimates.

#### Load data

```{r}
#| eval: false

library(Maaslin2)

metadata <- read.csv("DA_metadata.csv", row.names=1, check.names=FALSE)
counts_full <- read.csv("DA_counts_relative.csv", check.names=FALSE)

# Separate taxonomy
taxonomy_cols <- c("Prevalence in DNA extraction/NTC negative controls",
                   "Prevalence in Paraf. Controls",
                   "BactID", "domain", "phylum", "class", "order", 
                   "family", "genus", "species")
taxonomy <- counts_full[, taxonomy_cols]
counts <- counts_full[, !names(counts_full) %in% taxonomy_cols]
rownames(counts) <- taxonomy$BactID

```

#### Prepare Metadata

```{r}
#| eval: false

metadata$Group <- factor(metadata$Group, levels=c("Tumor", "NAT"))
metadata$Center <- factor(metadata$Center)
metadata$Patient_ID <- factor(metadata$Patient_ID)

```

#### Transpose Data

```{r}
#| eval: false

# MaAsLin2 needs: rows=samples, columns=taxa
counts_transposed <- as.data.frame(t(counts))

# Verify alignment
cat(sprintf("Sample alignment check: %s\n", 
            ifelse(all(rownames(counts_transposed) == rownames(metadata)), 
                   "✓ OK", "✗ FAILED")))

```

Maaslin requries samples as rows and features as columns hence we transpose it.

#### Run MaasLin

```{r}
#| eval: false

fit_data <- Maaslin2(
  input_data = counts_transposed,
  input_metadata = metadata,
  output = "maaslin2_output",
  min_abundance = 0.0,
  min_prevalence = 0.0,
  min_variance = 0.0,
  normalization = "NONE",          # Already relative abundances
  transform = "LOG",                # Log transformation for linear model
  analysis_method = "LM",           # Linear model
  max_significance = 0.25,          # Report if qval < 0.25
  random_effects = c("Patient_ID"), # ← HANDLES PAIRING
  fixed_effects = c("Center", "Group"),
  correction = "BH",                # Benjamini-Hochberg FDR
  standardize = TRUE,
  cores = 1,
  plot_heatmap = TRUE,
  heatmap_first_n = 50,
  plot_scatter = TRUE,
  max_pngs = 10,
  save_scatter = FALSE,
  save_models = FALSE,
  reference = "Group,Tumor"         # Tumor as reference
)

```

We put normlization as None because data is already relative abundance so , we dont re-normlaize. We add Patient ID as random effect to account for pairing.

#### Extract Result

```{r}
#| eval: false

# MaAsLin2 outputs results for ALL fixed effects (Group AND Center)
# Filter to Group only for main results

results <- read.csv("maaslin2_output/all_results.tsv", sep="\t")
group_only <- results[results$metadata == "Group", ]
write.csv(group_only, "maaslin2_group_only_results.csv", row.names=FALSE)

# Significant only
sig_results <- read.csv("maaslin2_output/significant_results.tsv", sep="\t")
sig_group_only <- sig_results[sig_results$metadata == "Group", ]
write.csv(sig_group_only, "maaslin2_group_significant.csv", row.names=FALSE)


```

As fixed effect it output for both center and group effect. Since we are only focused on group effect that is NAT vs T , we filter it.

#### Understanding output

![](images/clipboard-2193975865.png)

`feature` \| Bacterial taxon (BactID) \| Which bacteria

`metadata` \| Variable tested \| "Group" or "Center"

`value` \| Specific level being compared \| "NAT" (vs reference "Tumor")

`coef` \| Effect size (coefficient) \| How much higher/lower in NAT vs Tumor

`stderr` \| Standard error \| Uncertainty of estimate

`pval` \| P-value \| Statistical significance

`qval` \| FDR-adjusted q-value \| Multiple testing corrected

`N` \| Sample size \| Number of samples in analysis

`N.not.0` \| Non-zero samples \| How many samples had this bacteria

### 3.3.03 ALDEx2

ALDEx2 is used to identify differentially abundant bacteria while explicitly addressing the compositionality problem in sequencing data.

#### The compositionality problem

Because sequencing provides relative, not absolute, abundances, all taxa are constrained to sum to one. A change in one taxon therefore alters the proportions of others, creating a compositionality problem that traditional statistical models cannot handle properly.

To address this, ALDEx2 models the observed count data as one possible realization of a Dirichlet–multinomial distribution and performs Monte Carlo sampling to account for technical uncertainty. For each sample, it generates multiple simulated instances (typically 128), each representing a plausible composition of counts. Each simulated instance is then transformed using the centered log-ratio (CLR) transformation:

The centered log-ratio (CLR) transformation is defined as:

$$
\mathrm{CLR}(x_i) = \log \left( \frac{x_i}{\text{geometric mean of all } x_i} \right )
$$

This transformation removes the unit-sum constraint and allows meaningful comparison of taxa across samples.

ALDEx2 then performs statistical testing on each Monte Carlo instance usually Welch’s t-test for two-group comparisons such as Tumor vs NAT and computes corresponding effect sizes. These results are averaged across all instances to produce robust estimates of fold change and significance that incorporate sequencing uncertainty.

When covariates (e.g., Center) need to be included, ALDEx2 uses a generalized linear model of the form.

The CLR-transformed abundance is modeled as:

$$
\mathrm{CLR}(\text{abundance}) = \beta_0 + \beta_{\text{Center}} + \beta_{\text{Group}}
$$

fitted separately to each Monte Carlo instance, with coefficients averaged afterward.

Input data must be **integer count matrices** (features × samples), not relative abundances, because the Dirichlet-multinomial model requires count-like data. ALDEx2 therefore typically uses files such as `DA_counts_integer.csv` and `DA_metadata.csv`.

#### Load Data

```{r}
#| eval: false

library(ALDEx2)

metadata <- read.csv("DA_metadata.csv", row.names=1, check.names=FALSE)
counts_full <- read.csv("DA_counts_integer.csv", check.names=FALSE)

# Separate taxonomy
taxonomy_cols <- c("Prevalence in DNA extraction/NTC negative controls",
                   "Prevalence in Paraf. Controls",
                   "BactID", "domain", "phylum", "class", "order", 
                   "family", "genus", "species")
taxonomy <- counts_full[, taxonomy_cols]
counts <- counts_full[, !names(counts_full) %in% taxonomy_cols]
rownames(counts) <- taxonomy$BactID
```

#### Prepare Metadata

```{r}
#| eval: false

metadata$Group <- factor(metadata$Group, levels=c("Tumor", "NAT"))
metadata$Center <- factor(metadata$Center)

# Verify alignment
if (!identical(colnames(counts), rownames(metadata))) {
  stop("ERROR: Sample order doesn't match!")
}

```

#### Create Model Matrix(for GLM with Center adjustment)

```{r}
#| eval: false

mm <- model.matrix(~ Center + Group, data = metadata)
```

This converts factors into numeric design matrix used for GLM regression.

#### Run Aldex

```{r}
#| eval: false

aldex_clr <- aldex.clr(counts, mm, mc.samples=128, denom="all")
```

-   `counts` = integer count matrix

<!-- -->

-   `mm` = model matrix (for GLM)

<!-- -->

-   `mc.samples=128` = number of Monte Carlo instances

-   `denom="all"` = use geometric mean of all features for CLR

What the code does:

1.  For each sample, draws 128 Dirichlet replicates (each sums to 1)

2.  Applies CLR transformation to each replicate

3.  Result: 128 CLR-transformed versions of each feature per sample

#### Run GLM (adjust for center)

```{r}
#| eval: false

aldex_glm <- aldex.glm(aldex_clr, mm)
```

1.  For each MC instance, fits GLM: `CLR ~ Center + Group`

2.  Extracts coefficients for Group effect

3.  Averages coefficients across 128 instances

4.  Tests if Group coefficient significantly different from 0

#### Extract Result

```{r}
#| eval: false

aldex_results <- data.frame(aldex_glm)
aldex_results$BactID <- rownames(aldex_results)

# Extract Group effect columns
# Column names: "GroupNAT.Est", "GroupNAT.pval", "GroupNAT.pval.padj"
aldex_results$coef_Group_adjusted <- aldex_results$GroupNAT.Est
aldex_results$pval_Group <- aldex_results$GroupNAT.pval
aldex_results$padj_Group <- aldex_results$GroupNAT.pval.padj

```

#### Merge with taxonomy and save

```{r}
#| eval: false

aldex_results_tax <- merge(aldex_results, taxonomy, by="BactID")
aldex_results_tax <- aldex_results_tax[order(aldex_results_tax$padj_Group), ]

write.csv(aldex_results_tax, "aldex2_glm_results.csv", row.names=FALSE)

# Significant results
sig_005 <- aldex_results_tax[aldex_results_tax$padj_Group < 0.05 & 
                              !is.na(aldex_results_tax$padj_Group), ]
sig_010 <- aldex_results_tax[aldex_results_tax$padj_Group < 0.1 & 
                              !is.na(aldex_results_tax$padj_Group), ]

if (nrow(sig_005) > 0) {
  write.csv(sig_005, "aldex2_glm_significant_padj005.csv", row.names=FALSE)
}
if (nrow(sig_010) > 0) {
  write.csv(sig_010, "aldex2_glm_significant_padj010.csv", row.names=FALSE)
}

```

#### Understanding output

![](images/clipboard-2313902738.png)

`GroupNAT.Est` \| GLM coefficient for Group \| Center-adjusted log-ratio difference (NAT vs Tumor)

`GroupNAT.SE` \| Standard error \| Uncertainty of coefficient

`GroupNAT.t.val` \| T-statistic \| `coef / SE`

`GroupNAT.pval` \| P-value \| Significance of Group effect

`GroupNAT.pval.padj` \| Adjusted p-value \| FDR-corrected

#### Interpretation example:

GroupNAT.Est = 1.2 GroupNAT.pval.padj = 0.003

Interpretation: - After adjusting for Center - After averaging across 128 Monte Carlo instances - This bacterium has **1.2 units higher CLR-transformed abundance** in NAT vs Tumor - Highly significant (padj \< 0.05) - Positive = enriched in NAT - Negative = enriched in Tumor

### 3.3.04 ANCOM BC2

ANCOM-BC2 identifies differentially abundant bacteria through a bias-corrected log-linear modeling framework that accounts for both compositional effects and technical biases common in sequencing data. It improves on earlier methods like ANCOM by explicitly estimating and correcting for two major sources of bias sampling fraction bias and taxon-specific efficiency bias making it generally more powerful than ANCOM and less conservative than ALDEx2. ANCOM-BC2 can include covariates such as sequencing center but cannot handle random effects or paired designs.

The first bias, **sample-level bias** (or sampling-fraction bias), arises because different samples capture different total fractions of the microbial community. Even if relative proportions between taxa remain constant, differences in DNA extraction efficiency or sequencing depth mean some samples contain far more total reads than others. ANCOM-BC2 corrects this by estimating a sample-specific normalization factor, δᵢ, that adjusts each sample’s counts to a common scale.

The second bias, **taxon-level bias**, reflects the fact that some bacterial taxa are inherently harder to detect due to biological or technical properties for example, variation in cell wall lysis, GC content, or primer compatibility. These systematic detection differences are modeled through a taxon-specific bias term, νⱼ, which adjusts for consistent over- or under-representation across samples.

Mathematically, the observed count for taxon *j* in sample *i* can be expressed as

$$
y_{ij} = \mathrm{true\_abundance}_{ij} \times \delta_i \times \nu_j
$$

ANCOM-BC2 estimates both bias terms ($\delta_i$ and $\nu_j$) and removes them to recover bias-corrected abundances:

$$
\log(\mathrm{corrected\_abundance}_{ij}) = \log(y_{ij}) - \log(\delta_i) - \log(\nu_j)
$$

A bias-corrected log-linear model is then fitted to these corrected abundances:

$$
\log(\mathrm{corrected\_abundance}) = \beta_0 + \beta_{\text{Center}} + \beta_{\text{Group}}
$$

Here, $\beta_{\text{Group}}$ represents the estimated log fold-change between groups (e.g., Tumor vs NAT) **after removing both sampling and taxon-specific biases**.\
The resulting lfc value provides a bias-adjusted measure of differential abundance, and statistical significance is assessed on this corrected scale.

For input, ANCOM-BC2 requires integer count data with features (taxa) as rows and samples as columns, alongside a metadata data frame describing sample covariates. The standard input files are `DA_counts_integer.csv` and `DA_metadata.csv`.

#### Load Data

```{r}
#| eval: false

library(ANCOMBC)

metadata <- read.csv("DA_metadata.csv", row.names=1, check.names=FALSE)
counts_full <- read.csv("DA_counts_integer.csv", check.names=FALSE)

# Separate taxonomy
taxonomy_cols <- c("Prevalence in DNA extraction/NTC negative controls",
                   "Prevalence in Paraf. Controls",
                   "BactID", "domain", "phylum", "class", "order", 
                   "family", "genus", "species")
taxonomy <- counts_full[, taxonomy_cols]
counts <- counts_full[, !names(counts_full) %in% taxonomy_cols]
rownames(counts) <- taxonomy$BactID

```

#### Prepare Metadata

```{r}
#| eval: false

metadata$Group <- factor(metadata$Group, levels=c("Tumor", "NAT"))
metadata$Center <- factor(metadata$Center)

# Verify alignment
if (!identical(colnames(counts), rownames(metadata))) {
  stop("ERROR: Sample order doesn't match!")
}
```

#### Run ANCOM

```{r}
#| eval: false

ancombc2_result <- ancombc2(
  data = counts,
  assay_name = NULL,
  tax_level = NULL,
  fix_formula = "Group + Center",    # Fixed effects
  rand_formula = NULL,                # No random effects
  p_adj_method = "BH",                # Benjamini-Hochberg
  pseudo_sens = TRUE,                 # Sensitivity analysis
  prv_cut = 0,                        # No prevalence filter (already done)
  lib_cut = 0,                        # No library size filter
  s0_perc = 0.05,                     # Small constant for stability
  group = "Group",                    # Primary variable
  struc_zero = FALSE,                 # No structural zeros
  neg_lb = FALSE,                     # No negative lower bound
  alpha = 0.05,                       # Significance threshold
  n_cl = 1,                           # Number of cores
  verbose = TRUE,
  global = FALSE,                     # No global test
  pairwise = FALSE,                   # No pairwise (only 2 groups)
  dunnet = FALSE,                     # No Dunnett test
  trend = FALSE,                      # No trend test
  iter_control = list(tol = 1e-2, max_iter = 20, verbose = FALSE),
  em_control = list(tol = 1e-5, max_iter = 100),
  lme_control = NULL,
  mdfdr_control = NULL,
  trend_control = NULL,
  meta_data = metadata
)

```

#### Extract Result

```{r}
#| eval: false

results_df <- ancombc2_result$res

# Rename taxon to BactID for merging
results_df$BactID <- results_df$taxon

# Merge with taxonomy
results_tax <- merge(results_df, taxonomy, by="BactID", all.x=TRUE)

# Sort by q-value
results_tax <- results_tax[order(results_tax$q_GroupNAT), ]
```

#### Get sigificant results

```{r}
#| eval: false

# By q-value thresholds
sig_005 <- results_tax[results_tax$q_GroupNAT < 0.05 & 
                       !is.na(results_tax$q_GroupNAT), ]
sig_010 <- results_tax[results_tax$q_GroupNAT < 0.1 & 
                       !is.na(results_tax$q_GroupNAT), ]

# By ANCOM-BC2 declaration (accounts for effect size)
sig_declared <- results_tax[results_tax$diff_GroupNAT == TRUE & 
                            !is.na(results_tax$diff_GroupNAT), ]

# Robust significant (passed sensitivity analysis)
sig_robust <- results_tax[results_tax$diff_robust_GroupNAT == TRUE & 
                          !is.na(results_tax$diff_robust_GroupNAT), ]


```

#### Save result

```{r}
#| eval: false


# All results
write.csv(results_tax, "ancombc2_results.csv", row.names=FALSE)

# Significant by q-value
if (nrow(sig_005) > 0) {
  write.csv(sig_005, "ancombc2_significant_q005.csv", row.names=FALSE)
}
if (nrow(sig_010) > 0) {
  write.csv(sig_010, "ancombc2_significant_q010.csv", row.names=FALSE)
}

# ANCOM-BC2 declared significant
if (nrow(sig_declared) > 0) {
  write.csv(sig_declared, "ancombc2_significant_declared.csv", row.names=FALSE)
}

# Robust significant (most conservative)
if (nrow(sig_robust) > 0) {
  write.csv(sig_robust, "ancombc2_significant_robust.csv", row.names=FALSE)
}
```

#### Understanding the output file

![](images/clipboard-3221448154.png)

`taxon` \| Bacterial taxon \| Which bacteria

`lfc_GroupNAT` \| Log fold change \| Bias-corrected difference (NAT vs Tumor)\<br\>Positive = enriched in NAT

`se_GroupNAT` \| Standard error \| Uncertainty of LFC

`W_GroupNAT` \| Wald statistic \| `lfc / se`\<br\>Larger = stronger evidence

`p_GroupNAT` \| P-value \| Raw significance

`q_GroupNAT` \| Q-value \| FDR-adjusted (use this for significance)

`diff_GroupNAT` \| Logical (TRUE/FALSE) \| ANCOM-BC2's declaration of significance

`passed_ss_GroupNAT` \| Logical \| Passed sample size filter?

`diff_robust_GroupNAT` \| Logical \| Robust to sensitivity analysis?\<br\>**Most conservative**

**Also includes:**

-   `lfc_(Intercept)` = baseline (Tumor at reference Center)

    `lfc_CenterRambam`, `lfc_CenterSheba` = Center effects (adjustments)

**Log Fold Change (lfc):**

-   **lfc** \| **Interpretation** \| **Fold Change**

    +1 \| 2× higher in NAT \| 2

    -1 \| 2× lower in NAT (2× higher in Tumor) \| 0.5

    +2 \| 4× higher in NAT \| 4

    0 \| No difference \| 1
